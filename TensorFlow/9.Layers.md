## LAYERS

Layers - building blocks that make up the structure of the model. Each layer consists of a set of interconnected nodes, also known as neurons or units, which process the input data and produce output. 

We use layers to learn complex patterns and relationships in the data. Each layer learns and represents different aspects or levels of abstraction. By stacking layers, the network can progressively extract higher-level features from the input data, leading to more sophisticated and nuanced representations.
Imagine you have a stack of transparent sheets, where each sheet represents a layer in a neural network. Each sheet has a specific purpose and contributes to the overall transformation of the input data.
The bottom sheet represents the input layer, which takes the raw input data (e.g., images, text, or numerical features) and passes it to the next layer. It serves as the entry point for the information.
The intermediate sheets represent the hidden layers, which are responsible for processing and transforming the input data. Each sheet performs computations on the input it receives, applying mathematical operations and activation functions to extract useful features or representations.
The top sheet represents the output layer, which produces the final output of the neural network. It provides the predictions or classifications based on the information learned from the previous layers.
The information flows from the input layer, through the hidden layers, and finally to the output layer, just as the light passes through each transparent sheet in the stack.

__Hidden Layers__

Hidden layers allow neural networks to perform non-linear transformations on the input data, allowing neural networks to can capture and represent non-linear patterns and relationships in the data. This enables them to solve more complex problems that may have non-linear dependencies between input and output.
Each hidden layer in a neural network learns to extract relevant features or representations from the input data. As information passes through the hidden layers, these layers identify and amplify important patterns or characteristics in the data, progressively learning higher-level and more abstract features. 
Neural networks with multiple hidden layers can learn hierarchical representations of the data. Each layer captures different levels of abstraction, with lower layers focusing on basic features and higher layers capturing more complex concepts. This hierarchical structure enables the network to learn increasingly sophisticated and nuanced representations as information flows through the layers.
__examples__

Below, we define input placeholders `x` for the image data and `y_true` for the true labels.

We specify the number of hidden units (256 in this case) and create a hidden layer using `tf.layers.dense()`. This layer takes the input tensor x and applies a fully connected transformation with `ReLU` activation.

We add an output layer using `tf.layers.dense()`. This layer takes the output of the hidden layer as input and has 10 units (corresponding to the number of classes in the classification task). We set the activation to None to obtain the raw logits.

We initialize the variables using `tf.global_variables_initializer()`

This code provides a basic example of how to incorporate layers into a TensorFlow model. The hidden layer and output layer allow the network to learn non-linear representations and make predictions based on the input data. By stacking layers, the model can capture more complex patterns and improve its ability to generalize to unseen data.

```
import tensorflow as tf

# Create a computational graph
x = tf.placeholder(tf.float32, shape=[None, 784])  # Input placeholder for image data
y_true = tf.placeholder(tf.float32, shape=[None, 10])  # Placeholder for true labels

# Define the neural network architecture
hidden_units = 256
hidden_layer = tf.layers.dense(inputs=x, units=hidden_units, activation=tf.nn.relu)
output_layer = tf.layers.dense(inputs=hidden_layer, units=10, activation=None)

# Initialize variables
init = tf.global_variables_initializer()

# Create a TensorFlow session and run the graph
with tf.Session() as sess:
    sess.run(init)  # Initialize the variables

    # Perform inference
    output = sess.run(output_layer, feed_dict={x: test_images})

    # Print the output
    print("Output:", output)
```

__some new terms__

`logit` refers to the unnormalized predicted value or score generated by the output layer of a classification model before applying a `softmax activation` function. `Logits` represent the raw, pre-activation values produced by the model for each class.

To understand logits, let's consider a multi-class classification scenario where we want to classify an input into one of several classes. After processing the input through the neural network, the output layer generates a vector of scores or logits, where each element represents the model's confidence or strength of association with a particular class. The higher the logit value, the stronger the model's belief that the input belongs to that class.

It's important to note that logits are not probabilities because they are not constrained within any specific range. They can be positive, negative, or zero. To convert logits into probabilities, the `softmax function` is commonly applied. Softmax takes the vector of logits and normalizes them, transforming them into a probability distribution over the classes, where each class probability represents the model's estimated likelihood for that class.

__Activation Functions__

`activation functions` are applied to the outputs of individual neurons or units within a layer. They introduce non-linearity into the network, allowing it to learn and model complex patterns in the data. Here are explanations of some commonly used activation functions:

`Sigmoid Activation (Logistic Activation)`:
The sigmoid activation function is defined as `f(x) = 1 / (1 + exp(-x))`. It maps the input to a range between 0 and 1. It is useful in binary classification problems where the output needs to be interpreted as probabilities. However, sigmoid functions suffer from the vanishing gradient problem and are less commonly used in deep neural networks.

Hyperbolic Tangent Activation (Tanh):
The hyperbolic tangent activation function is defined as f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)). It maps the input to a range between -1 and 1. It exhibits similar properties to the sigmoid function but is symmetric around the origin. Tanh activation is commonly used in hidden layers of neural networks.

`Rectified Linear Unit (ReLU)`:
ReLU activation is defined as `f(x) = max(0, x)`, which means it outputs the input as is if it is positive and clips it to 0 if it is negative. ReLU has become popular due to its simplicity and effectiveness in deep learning. It helps with faster convergence during training and mitigates the vanishing gradient problem. However, ReLU can lead to dead neurons (zeros) during training if the learning rate is not properly adjusted.

`Leaky ReLU`:
Leaky ReLU is an improved version of the ReLU activation function. It has a small slope for negative values, allowing a small gradient to pass through when the input is negative. This mitigates the issue of dead neurons encountered in ReLU. The function is defined as f(x) = max(0.01x, x), where 0.01 is a small constant.

`Softmax Activation:`
The softmax activation function is commonly used in the output layer of classification problems. It transforms the inputs into a probability distribution over multiple classes. It ensures that the sum of the outputs is 1, making it suitable for multi-class classification tasks. Softmax is defined as f(x) = exp(x) / sum(exp