### TensorFlow

**What is a tensor?**

Tensor - a data structure that represents a multi-dimensional array of elements.

**What are some Tensor attributes**

Rank - number of dimensions it has (a scalar has a rank of 0, a vector 1, a matrix 2, a tensor 3+. (3D tensors, or rows of 3D would be 4D).  Higher rank tensors are usted in image-processing, video analysis, etc. 

Shape - the size of each dimension - the number of elements within each axis. A shape of (3, 4) has 3 rows and 4 columns

Data type - elements in a tensor array can be of a certain data type: string, int64, etc

__What are some operations performed on a tensor__
math (add, multiply)
Element-wise operations
Matrix operations

__Other terms__

Computational Graphs - represents the operations and flow of data between tensors.  Tensors flow through the graph as operations are performed on them to product output tensors. 

TensorFlow session - the execution of computational graph that will obtain the result of our work. We must create the session `tf.Session() as ses` and run it `ses.run()`. 

__The basic structure of a TensorFlow program__

1. create tensors
2. Build computational graphs
3. Run session to execute the graph and obtain results.

```
import tensorflow as tf

# Create a computational graph
a = tf.constant(5)  # Create a constant tensor with value 5
b = tf.constant(3)  # Create a constant tensor with value 3
c = tf.add(a, b)    # Add the tensors a and b

# Run a TensorFlow session to execute the graph
with tf.Session() as sess:
    result = sess.run(c)
    print("The result of adding a and b is:", result)
```

__Level 2__

Tensors can be constants or mutable (able to be transformed).  

Placeholders - an empty tensor that will be fed with actual data later.  You could define is it as a float type thus: `tf.float32`.  

Variables - tensors whose values can be changed during execution of a computational graph.  This var must be initialized first.  To do initialize all the variables in a graph, use `tf.global_variables_initializer()` 

Below, we evaluate the comp. graph multiple times with different values for `a` 

We can evaluate the graph multiple time with different values for `a` by adding the `feed_dict` argument, wihc provides actual values for the placeholder: `a`

```
import tensorflow as tf

# Create a computational graph
a = tf.placeholder(tf.float32)  # Create a placeholder for a tensor of type float32
b = tf.Variable(3.0)  # Create a variable tensor with an initial value of 3.0
c = tf.add(a, b)  # Add the tensors a and b

# Initialize variables
init = tf.global_variables_initializer()

# Run a TensorFlow session to execute the graph

    with tf.Session() as sess:
    sess.run(init)  # Initialize the variables

    # Evaluate the computational graph with different values for a
    result1 = sess.run(c, feed_dict={a: 5.0})
    result2 = sess.run(c, feed_dict={a: 10.0})

    print("The result of adding a and b with a=5.0 is:", result1)
    print("The result of adding a and b with a=10.0 is:", result2)
```
Concept set three

`Variable Size Placeholder` - We modify the placeholder a by specifying `shape=[None]`, which allows the tensor to have a variable size. By setting the shape to [None], we indicate that the tensor can have any number of elements along the first dimension. This flexibility allows us to feed tensors of different lengths to the computational graph.

`Variable Initialization with an Array`: Instead of initializing the variable `b` with a single value, we now initialize it with an array of values: `[1.0, 2.0, 3.0]`. This creates a variable tensor with multiple elements.

`Element-wise Addition and Mean`: In addition to adding tensors `a` and `b` element-wise, we introduce the `tf.reduce_mean()` function to calculate the mean of the resulting tensor `c`. The `tf.reduce_mean()` operation reduces the tensor to a single scalar value by computing the mean of all its elements.

`Element-wise` is an operation performed independnetly on each element of one or more tensors, without any interaction between the element.  This results in a new tensor of the same shape. 

```
A = [1, 2, 3]
B = [4, 5, 6]
```
Element-wise addition between A and B would result in a new tensor C where each element is the sum of the corresponding elements of A and B:

`C = [1 + 4, 2 + 5, 3 + 6] = [5, 7, 9]`

`Running Multiple Operations`: We modify the `sess.run()` call to evaluate multiple tensors simultaneously. By passing a list of tensors `[c, mean_c], `we obtain the values of both c and mean_c in a single execution.

```import tensorflow as tf

# Create a computational graph
a = tf.placeholder(tf.float32, shape=[None])  # Create a placeholder for a tensor with variable size
b = tf.Variable([1.0, 2.0, 3.0])  # Create a variable tensor with an initial array value
c = tf.add(a, b)  # Add the tensors a and b element-wise
mean_c = tf.reduce_mean(c)  # Calculate the mean of tensor c

# Initialize variables
init = tf.global_variables_initializer()

# Run a TensorFlow session to execute the graph
with tf.Session() as sess:
    sess.run(init)  # Initialize the variables

    # Evaluate the computational graph with different values for a
    result1, mean1 = sess.run([c, mean_c], feed_dict={a: [2.0, 3.0, 4.0]})
    result2, mean2 = sess.run([c, mean_c], feed_dict={a: [5.0, 6.0, 7.0, 8.0, 9.0]})

# We print the results of c and mean_c for each evaluation to observe the element-wise addition and the mean of the resulting tensor.

    print("Result 1 (c):", result1)
    print("Mean of Result 1 (mean_c):", mean1)
    print("Result 2 (c):", result2)
    print("Mean of Result 2 (mean_c):", mean2)
```
## LAYERS

Layers - building blocks that make up the structure of the model. Each layer consists of a set of interconnected nodes, also known as neurons or units, which process the input data and produce output. We use layers to learn complex patterns and relationships in the data. Each layer learns and represents different aspects or levels of abstraction. By stacking layers, the network can progressively extract higher-level features from the input data, leading to more sophisticated and nuanced representations.
Imagine you have a stack of transparent sheets, where each sheet represents a layer in a neural network. Each sheet has a specific purpose and contributes to the overall transformation of the input data.
The bottom sheet represents the input layer, which takes the raw input data (e.g., images, text, or numerical features) and passes it to the next layer. It serves as the entry point for the information.
The intermediate sheets represent the hidden layers, which are responsible for processing and transforming the input data. Each sheet performs computations on the input it receives, applying mathematical operations and activation functions to extract useful features or representations.
The top sheet represents the output layer, which produces the final output of the neural network. It provides the predictions or classifications based on the information learned from the previous layers.
The information flows from the input layer, through the hidden layers, and finally to the output layer, just as the light passes through each transparent sheet in the stack.

__Hidden Layers__

Hidden layers allow neural networks to perform non-linear transformations on the input data, allowing neural networks to can capture and represent non-linear patterns and relationships in the data. This enables them to solve more complex problems that may have non-linear dependencies between input and output.
Each hidden layer in a neural network learns to extract relevant features or representations from the input data. As information passes through the hidden layers, these layers identify and amplify important patterns or characteristics in the data, progressively learning higher-level and more abstract features. 
Neural networks with multiple hidden layers can learn hierarchical representations of the data. Each layer captures different levels of abstraction, with lower layers focusing on basic features and higher layers capturing more complex concepts. This hierarchical structure enables the network to learn increasingly sophisticated and nuanced representations as information flows through the layers.
__examples__

Below, we define input placeholders `x` for the image data and `y_true` for the true labels.

We specify the number of hidden units (256 in this case) and create a hidden layer using `tf.layers.dense()`. This layer takes the input tensor x and applies a fully connected transformation with `ReLU` activation.

We add an output layer using `tf.layers.dense()`. This layer takes the output of the hidden layer as input and has 10 units (corresponding to the number of classes in the classification task). We set the activation to None to obtain the raw logits.

We initialize the variables using `tf.global_variables_initializer()`

This code provides a basic example of how to incorporate layers into a TensorFlow model. The hidden layer and output layer allow the network to learn non-linear representations and make predictions based on the input data. By stacking layers, the model can capture more complex patterns and improve its ability to generalize to unseen data.

```
import tensorflow as tf

# Create a computational graph
x = tf.placeholder(tf.float32, shape=[None, 784])  # Input placeholder for image data
y_true = tf.placeholder(tf.float32, shape=[None, 10])  # Placeholder for true labels

# Define the neural network architecture
hidden_units = 256
hidden_layer = tf.layers.dense(inputs=x, units=hidden_units, activation=tf.nn.relu)
output_layer = tf.layers.dense(inputs=hidden_layer, units=10, activation=None)

# Initialize variables
init = tf.global_variables_initializer()

# Create a TensorFlow session and run the graph
with tf.Session() as sess:
    sess.run(init)  # Initialize the variables

    # Perform inference
    output = sess.run(output_layer, feed_dict={x: test_images})

    # Print the output
    print("Output:", output)
    ```

__some new terms__

`logit` refers to the unnormalized predicted value or score generated by the output layer of a classification model before applying a `softmax activation` function. `Logits` represent the raw, pre-activation values produced by the model for each class.

To understand logits, let's consider a multi-class classification scenario where we want to classify an input into one of several classes. After processing the input through the neural network, the output layer generates a vector of scores or logits, where each element represents the model's confidence or strength of association with a particular class. The higher the logit value, the stronger the model's belief that the input belongs to that class.

It's important to note that logits are not probabilities because they are not constrained within any specific range. They can be positive, negative, or zero. To convert logits into probabilities, the `softmax function` is commonly applied. Softmax takes the vector of logits and normalizes them, transforming them into a probability distribution over the classes, where each class probability represents the model's estimated likelihood for that class.

__Activation Functions__

`activation functions` are applied to the outputs of individual neurons or units within a layer. They introduce non-linearity into the network, allowing it to learn and model complex patterns in the data. Here are explanations of some commonly used activation functions:

`Sigmoid Activation (Logistic Activation)`:
The sigmoid activation function is defined as `f(x) = 1 / (1 + exp(-x))`. It maps the input to a range between 0 and 1. It is useful in binary classification problems where the output needs to be interpreted as probabilities. However, sigmoid functions suffer from the vanishing gradient problem and are less commonly used in deep neural networks.

Hyperbolic Tangent Activation (Tanh):
The hyperbolic tangent activation function is defined as f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)). It maps the input to a range between -1 and 1. It exhibits similar properties to the sigmoid function but is symmetric around the origin. Tanh activation is commonly used in hidden layers of neural networks.

`Rectified Linear Unit (ReLU)`:
ReLU activation is defined as `f(x) = max(0, x)`, which means it outputs the input as is if it is positive and clips it to 0 if it is negative. ReLU has become popular due to its simplicity and effectiveness in deep learning. It helps with faster convergence during training and mitigates the vanishing gradient problem. However, ReLU can lead to dead neurons (zeros) during training if the learning rate is not properly adjusted.

`Leaky ReLU`:
Leaky ReLU is an improved version of the ReLU activation function. It has a small slope for negative values, allowing a small gradient to pass through when the input is negative. This mitigates the issue of dead neurons encountered in ReLU. The function is defined as f(x) = max(0.01x, x), where 0.01 is a small constant.

`Softmax Activation:`
The softmax activation function is commonly used in the output layer of classification problems. It transforms the inputs into a probability distribution over multiple classes. It ensures that the sum of the outputs is 1, making it suitable for multi-class classification tasks. Softmax is defined as f(x) = exp(x) / sum(exp
